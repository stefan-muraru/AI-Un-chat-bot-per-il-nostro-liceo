from ollama import chat, ChatResponse

with open("C:/Users/vital/Desktop/python/Liceo_Tosi_QA.txt", "r", encoding="utf-8") as file:
    contenuto_scuola = file.read()

print("Assistente del Liceo Tosi attivo. Scrivi 'esci' per uscire.\n")

while True:
    messaggio = input("Tu: ")
    if messaggio.strip().lower() in ["esci", "exit", "quit", "end", "fine", "stop"]:
        print("Conversazione terminata.")
        break

    prompt_completo = (
        "Agisci come un assistente scolastico del Liceo Tosi. Rispondi usando solo le informazioni seguenti, "
        "ma in modo **chiaro, completo, anche con elenchi se serve**, e con un tono amichevole ma professionale.\n\n"
        f"{contenuto_scuola}\n\n"
        f"Domanda: {messaggio}\n"
        "Risposta completa:"
    )

    messages = [
        {'role': 'user', 'content': prompt_completo}
    ]

    print("\nAssistente: ", end="", flush=True)
    try:
        for chunk in chat(model='gemma3', messages=messages, stream=True):
            if chunk and chunk.message and chunk.message.content:
                print(chunk.message.content, end="", flush=True)
    except Exception as e:
        print(f"\nErrore nella chiamata al modello: {e}")
    
    print("\n" + "-" * 60)
