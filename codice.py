from ollama import chat
from ollama import ChatResponse

# âœ… Caricamento sicuro del file con gestione errori
try:
    with open("C:/Users/vital/Desktop/python/Liceo_Tosi.txt", "r", encoding="utf-8") as file:
        contenuto_scuola = file.read()
except FileNotFoundError:
    print("âŒ Errore: Il file 'Liceo_Tosi.txt' non Ã¨ stato trovato.")
    exit()
except Exception as e:
    print(f"âŒ Errore nella lettura del file: {e}")
    exit()

print("âœ… Assistente del Liceo Tosi attivo. Scrivi 'esci' per uscire.\n")

# ğŸ” Inizio ciclo domanda/risposta
while True:
    messaggio = input("Tu: ")

    if messaggio.strip().lower() in ["esci", "exit", "quit"]:
        print("ğŸ›‘ Conversazione terminata.")
        break

    # ğŸ” Costruiamo un prompt completo includendo il contenuto del file
    prompt_completo = (
        "Sei un assistente virtuale del Liceo Tosi. Rispondi solo usando le informazioni seguenti:\n\n"
        f"{contenuto_scuola}\n\n"
        f"Domanda: {messaggio}\n"
        "Risposta:"
    )

    # ğŸ‘‰ Tutto in un unico messaggio 'user'
    messages = [
        {'role': 'user', 'content': prompt_completo}
    ]

    try:
        response: ChatResponse = chat(model='deepseek-r1', messages=messages)
        risposta = response.message.content if response and response.message else "âŒ Nessuna risposta dal modello."
    except Exception as e:
        risposta = f"âŒ Errore nella chiamata al modello: {e}"

    print("\nAssistente:", risposta)
    print("-" * 60)
